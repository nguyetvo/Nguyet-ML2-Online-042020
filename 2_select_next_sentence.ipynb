{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "2 - select next sentence.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyetvo/Nguyet-ML2-Online-042020/blob/master/2_select_next_sentence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D17Q9xHKzFyj",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation using Bidirectional LSTM and Doc2Vec models 2/3\n",
        "\n",
        "If you have reached directly this page, I suggest to start reading the first part of this article. It describes how to create a RNN model to generate a text, word after word.\n",
        "\n",
        "I finished the first part of the article explaining I will try to improve the generation of sentences, by detecting patterns in the sequences of sentences, not only in the sequences of words.\n",
        "\n",
        "It could be an improvement, because doing that, the context of a paragraph (is it a description of a countryside? a dialog between characters? which people are involved? what are the previous actions? etc.) could emerge and can be used to select wisely the next sentence of the text.\n",
        "\n",
        "The process will be similar to the previous one, however, I will have to vectorize all sentences in the text, and try to find patterns in sequences of these vectors.\n",
        "\n",
        "In order to do that, we will use **Doc2Vec**.\n",
        "\n",
        "# 1. Doc2Vec\n",
        "Doc2Vec is able to vectorize a paragraph of text. If you do not know it, I suggest to have a look on the gensim web site, that describes how its work and what you’re allowed to do with it.\n",
        "\n",
        "In a short, we will transform each sentences of our text in a vector of a specific space. The great thing of the approach is we will be able to compare them ; by example, to retrieve the most similar sentence of a given one.\n",
        "\n",
        "Last but not least, the dimension of the vectors will be the same, whatever is the number of words in their linked sentence.\n",
        "\n",
        "It is exactly what we are looking for: I will be able to train a new LSTM, trying to catch pattern from sequences of vectors of the same dimensions.\n",
        "\n",
        "``I have to be honest: I am not sure we can perform such task with enough accuracy, but let’s have some tests. It is an experiment, at worst, it will be a good exercice.``\n",
        "\n",
        "So, once all sentences will be converted to vectors, we will try to **train a new bidirectional LSTM**. It purpose will be to predict the best vector, next to a sequence of vectors.\n",
        "\n",
        "Then how will we generate text ?\n",
        "\n",
        "Pretty easy: thanks to our previous LSTM model, we will generate sentences as candidates to be the next phrase. We will infer their vectors using the **trained doc2Vec model**, then pick the closest one to the prediction of our new LSTM model.\n",
        "\n",
        "## 1.1 Create the Doc2Vec Model\n",
        "The first task is to create our **doc2vec model**, dedicated to our text and embedded sentences.\n",
        "\n",
        "**Doc2Vec** assumes its input to be a list a words, with a label, per sentence:\n",
        "\n",
        "``Example: ['tobus', 'ouvre', 'la', 'porte', '.'] LABEL1``\n",
        "\n",
        "So we have to extract from the text each sentences and splits their words.\n",
        "\n",
        "by convention, I assume a sentence ends with “.”,”?”,”!”,”:” or “…”. The script reads each text, and create a new sentence each time it reaches on of these characters.\n",
        "\n",
        "First, we load the Doc2Vec library, we load our data and set some parameter:\n",
        "\n",
        "- all texts are stored in the **data_dir** directory,\n",
        "- the **file_list** list contains the names of all text files in the **data_dir** directory,\n",
        "- the **save_dir** will be used to save models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GJTMw_pzFyl",
        "colab_type": "code",
        "colab": {},
        "outputId": "7b86a902-351e-4e6b-e363-b2c7af07d3c3"
      },
      "source": [
        "#import gensim library\n",
        "import gensim\n",
        "from gensim.models.doc2vec import LabeledSentence\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import codecs\n",
        "\n",
        "#parameters\n",
        "data_dir = 'data/Artistes_et_Phalanges-David_Campion'# data directory containing input.txt\n",
        "save_dir = 'save' # directory to store models\n",
        "file_list = [\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"301\",\"302\",\"303\",\"304\",\"305\",\"306\",\"307\",\"308\",\"309\",\"310\",\"311\",\"312\",\"313\",\"314\",\"401\",\"402\",\"403\",\"404\",\"405\",\"406\",\"407\",\"408\",\"409\",\"410\",\"411\",\"412\"]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiG_PVVozFy4",
        "colab_type": "text"
      },
      "source": [
        "I create the list of sentences for the doc2vec model: to split easily sentences, I use the **spaCy** library. Then, I create the a list of Labels for these sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnAKBQM0zFy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#import spacy, and french model\n",
        "import spacy\n",
        "nlp = spacy.load('fr')\n",
        "\n",
        "#initiate sentences and labels lists\n",
        "sentences = []\n",
        "sentences_label = []\n",
        "\n",
        "#create sentences function:\n",
        "def create_sentences(doc):\n",
        "    ponctuation = [\".\",\"?\",\"!\",\":\",\"…\"]\n",
        "    sentences = []\n",
        "    sent = []\n",
        "    for word in doc:\n",
        "        if word.text not in ponctuation:\n",
        "            if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
        "                sent.append(word.text.lower())\n",
        "        else:\n",
        "            sent.append(word.text.lower())\n",
        "            if len(sent) > 1:\n",
        "                sentences.append(sent)\n",
        "            sent=[]\n",
        "    return sentences\n",
        "\n",
        "#create sentences from files\n",
        "for file_name in file_list:\n",
        "    input_file = os.path.join(data_dir, file_name + \".txt\")\n",
        "    #read data\n",
        "    with codecs.open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "    #create sentences\n",
        "    doc = nlp(data)\n",
        "    sents = create_sentences(doc)\n",
        "    sentences = sentences + sents\n",
        "    \n",
        "#create labels\n",
        "for i in range(np.array(sentences).shape[0]):\n",
        "    sentences_label.append(\"ID\" + str(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKv3aafBzFzG",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Train doc2vec model\n",
        "As explained above, **doc2vec** required its inputs to be correctly shaped. In order to do that, we define a specific class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fIC79yFCzFzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LabeledLineSentence(object):\n",
        "    def __init__(self, doc_list, labels_list):\n",
        "        self.labels_list = labels_list\n",
        "        self.doc_list = doc_list\n",
        "    def __iter__(self):\n",
        "        for idx, doc in enumerate(self.doc_list):\n",
        "            yield gensim.models.doc2vec.LabeledSentence(doc,[self.labels_list[idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELKeu2LczFzO",
        "colab_type": "text"
      },
      "source": [
        "I also create a specific function to train the doc2vec model. Its purpose is to update easily training paramaters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "P-rfOryfzFzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_doc2vec_model(data, docLabels, size=300, sample=0.000001, dm=0, hs=1, window=10, min_count=0, workers=8,alpha=0.024, min_alpha=0.024, epoch=15, save_file='./data/doc2vec.w2v') :\n",
        "    startime = time.time()\n",
        "    \n",
        "    print(\"{0} articles loaded for model\".format(len(data)))\n",
        "\n",
        "    it = LabeledLineSentence(data, docLabels)\n",
        "\n",
        "    model = gensim.models.Doc2Vec(size=size, sample=sample, dm=dm, window=window, min_count=min_count, workers=workers,alpha=alpha, min_alpha=min_alpha, hs=hs) # use fixed learning rate\n",
        "    model.build_vocab(it)\n",
        "    for epoch in range(epoch):\n",
        "        print(\"Training epoch {}\".format(epoch + 1))\n",
        "        model.train(it,total_examples=model.corpus_count,epochs=model.iter)\n",
        "        # model.alpha -= 0.002 # decrease the learning rate\n",
        "        # model.min_alpha = model.alpha # fix the learning rate, no decay\n",
        "        \n",
        "    #saving the created model\n",
        "    model.save(os.path.join(save_file))\n",
        "    print('model saved')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJq7U-4FzF0I",
        "colab_type": "text"
      },
      "source": [
        "few notes regarding the parameters of the function: the default parameters have been chosen empirically.\n",
        "\n",
        "Now, it's time to train the **doc2vec model**. Simply run the command:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ophKj77zF0J",
        "colab_type": "code",
        "colab": {},
        "outputId": "f1568cf2-acd4-4e24-ad6c-d0bbb6cb1bac"
      },
      "source": [
        "train_doc2vec_model(sentences, sentences_label, size=500,sample=0.0,alpha=0.025, min_alpha=0.001, min_count=0, window=10, epoch=20, dm=0, hs=1, save_file='./data/doc2vec.w2v')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12273 articles loaded for model\n",
            "Training epoch 1\n",
            "Training epoch 2\n",
            "Training epoch 3\n",
            "Training epoch 4\n",
            "Training epoch 5\n",
            "Training epoch 6\n",
            "Training epoch 7\n",
            "Training epoch 8\n",
            "Training epoch 9\n",
            "Training epoch 10\n",
            "Training epoch 11\n",
            "Training epoch 12\n",
            "Training epoch 13\n",
            "Training epoch 14\n",
            "Training epoch 15\n",
            "Training epoch 16\n",
            "Training epoch 17\n",
            "Training epoch 18\n",
            "Training epoch 19\n",
            "Training epoch 20\n",
            "model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTneQ_jCzF0i",
        "colab_type": "text"
      },
      "source": [
        "Here are some insights for the used parameters:\n",
        "\n",
        "- **dimensions**: 300 dimensions seem to work well for classic subjects. In my case, after few tests, I prefer to choose 500 dimensions,\n",
        "- **epochs**: below 10 epochs, results are not good enough (similiary are not working well), and bigger number of epochs creates to much similar vectors. So I chose 20 epochs for the training.\n",
        "- **min_count**: I want to integrate all words in the training, even those with very few occurence. Indeed, I assume that, for my exercice, specific words could be important. I set the value to 0, but 3 to 5 should be OK.\n",
        "- **sample**: *0.0*. I do not want to downsample randomly higher-frequency words, so I disabled it.\n",
        "- **hs and dm**: Each time I want to infer a new vector from the trained model, for a given sentence, I want to have the same output vector. In order to do that (strangly it’s not so intuitive), I need to use a distributed bag of words as *training algorithm (dm=0)* and *hierarchical softmax (hs=1)*. Indeed, for my purpose, distributive memory and negative sampling seems to give less good results.\n",
        "\n",
        "# 2. Create the Input Dataset\n",
        "First, using my trained **doc2Vec**, I will infer the vector for all sentences of my texts. The **doc2vec** model will provide directly the vector of each sentence, we just have to iterate over the whole sentences list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9bg9XiuzF0k",
        "colab_type": "code",
        "colab": {},
        "outputId": "09b5dd68-bdaf-4f96-fbb9-6ef1d9b7501c"
      },
      "source": [
        "#import library\n",
        "from six.moves import cPickle\n",
        "\n",
        "#load the model\n",
        "d2v_model = gensim.models.doc2vec.Doc2Vec.load('./data/doc2vec.w2v')\n",
        "\n",
        "sentences_vector=[]\n",
        "\n",
        "t = 500\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    if i % t == 0:\n",
        "        print(\"sentence\", i, \":\", sentences[i])\n",
        "        print(\"***\")\n",
        "    sent = sentences[i]\n",
        "    sentences_vector.append(d2v_model.infer_vector(sent, alpha=0.001, min_alpha=0.001, steps=10000))\n",
        "    \n",
        "#save the sentences_vector\n",
        "sentences_vector_file = os.path.join(save_dir, \"sentences_vector_500_a001_ma001_s10000.pkl\")\n",
        "with open(os.path.join(sentences_vector_file), 'wb') as f:\n",
        "    cPickle.dump((sentences_vector), f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence 0 : ['—', 'non', ',', 'celui-là', 'n’', 'y', 'parvient', 'pas', 'non', 'plus', '!']\n",
            "***\n",
            "sentence 500 : ['ce', 'bouclier', 'écarte', 'et', 'détourne', 'les', 'eaux', 'du', 'fleuve', ',', 'n’', 'en', 'laissant', 'filtrer', 'qu’', 'une', 'infime', 'partie', 'pour', 'l’', 'usage', 'des', 'citadins', '.']\n",
            "***\n",
            "sentence 1000 : ['essayez', 'de', 'distinguer', 'les', 'fils', 'du', 'temps', '.']\n",
            "***\n",
            "sentence 1500 : ['le', 'plafond', 'est', 'couvert', 'de', 'chevaux', 'au', 'galop', 'directement', 'peints', 'sur', 'la', 'pierre', '.']\n",
            "***\n",
            "sentence 2000 : ['—', 'je', 'ne', 'suis', 'pas', 'le', 'penangis', '…']\n",
            "***\n",
            "sentence 2500 : ['ce', 'que', 'je', 'souhaite', ',', 'c’', 'est', 'que', 'vous', 'soyez', 'prêts', 'au', 'moment', 'voulu', '.']\n",
            "***\n",
            "sentence 3000 : ['le', 'jeune', 'homme', 'hausse', 'les', 'épaules', 'et', 'se', 'dirige', 'vers', 'la', 'sortie', '.']\n",
            "***\n",
            "sentence 3500 : ['silvi', 'attrape', 'le', 'premier', 'livre', 'qui', 'lui', 'tombe', 'sous', 'la', 'main', '.']\n",
            "***\n",
            "sentence 4000 : ['de', 'nos', 'jours', ',', 'elles', 'ressemblent', 'plus', 'à', 'de', 'grandes', 'familles', '…']\n",
            "***\n",
            "sentence 4500 : ['elle', 'étouffe', 'un', 'juron', ',', 'tape', 'du', 'poing', 'sur', 'le', 'battant', 'et', 'lui', 'jette', 'un', 'grand', 'coup', 'de', 'pied', '.']\n",
            "***\n",
            "sentence 5000 : ['la', 'bête', 's’', 'accroupit', ',', 'la', 'tête', 'à', 'ras', 'le', 'sol', ',', 'tout', 'en', 'regardant', 'd’', 'un', 'œil', 'vicieux', 'les', 'deux', 'apprentis', '.']\n",
            "***\n",
            "sentence 5500 : ['nolan', 'jurerait', 'que', ',', 'l’', 'espace', 'd’', 'un', 'instant', ',', 'le', 'vieil', 'homme', 's’', 'est', 'senti', 'mal', 'à', 'l’', 'aise', '.']\n",
            "***\n",
            "sentence 6000 : ['la', 'petite', 'mara', 'descend', 'en', 'courant', 'le', 'reste', 'du', 'coteau', '.']\n",
            "***\n",
            "sentence 6500 : ['—', 'aujourd’hui', ',', 'c’', 'est', 'différent', '…']\n",
            "***\n",
            "sentence 7000 : ['écoute', '…']\n",
            "***\n",
            "sentence 7500 : ['le', 'novice', 'adossé', 'au', 'pilier', 'se', 'redresse', 'tout', 'à', 'coup', 'de', 'toute', 'sa', 'taille', ',', 'montre', 'du', 'doigt', 'quelque', 'chose', 'en', 'direction', 'des', 'toits', 'et', 'secoue', 'la', 'tête', '.']\n",
            "***\n",
            "sentence 8000 : ['alfan', 'vient', 'de', 'briser', 'la', 'discussion', 'de', 'la', 'bande', '.']\n",
            "***\n",
            "sentence 8500 : ['traîner', 'à', 'des', 'dizaines', 'de', 'mètres', 'sous', 'terre', ',', 'c’', 'est', 'pas', 'la', 'meilleure', 'des', 'choses', 'à', 'faire', ',', 'nous', 'devons', 'retourner', 'à', 'l’', 'air', 'libre', 'coûte', 'que', 'coûte', '!']\n",
            "***\n",
            "sentence 9000 : ['la', 'température', 'y', 'est', 'plus', 'clémente', ',', 'lothar', 'peut', 'désormais', 'dénouer', 'sa', 'grande', 'écharpe', '.']\n",
            "***\n",
            "sentence 9500 : ['sa', 'blessure', 'doit', 'être', 'plus', 'grave', 'qu’', 'elle', 'en', 'a', 'l’', 'air', 'et', 'nécessite', 'certainement', 'du', 'calme', 'et', 'beaucoup', 'de', 'repos', '.']\n",
            "***\n",
            "sentence 10000 : ['la', 'guerrière', 'se', 'montre', 'incertaine', 'puis', 'se', 'redresse', 'en', 'regardant', 'son', 'supérieur', 'droit', 'dans', 'les', 'yeux', '.']\n",
            "***\n",
            "sentence 10500 : ['verdande', 'chasse', 'ses', 'pensées', 'noires', '.']\n",
            "***\n",
            "sentence 11000 : ['il', 'attaque', 'les', 'rubans', 'de', 'l’', 'espace', 'qui', 'tressent', 'nos', 'corps', '.']\n",
            "***\n",
            "sentence 11500 : ['de', 'plus', ',', 'il', 'semble', 'qu’', 'elle', 'et', 'mara', 'ont', 'effectué', 'une', 'opération', 'dangereuse', 'et', 'formellement', 'défendue', '.']\n",
            "***\n",
            "sentence 12000 : ['—', 'si', ',', 'c’', 'est', 'possible', '…']\n",
            "***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo27gKHezF01",
        "colab_type": "text"
      },
      "source": [
        "Note: I do not use vectors generated during the training, because I want to compare them to vectors infered for sentences it did not seen. It’s better to generate them in the same way.\n",
        "\n",
        "Now, in order to create the Keras input data set **(X_train, y_train)**, we have to folow these guidelines:\n",
        "\n",
        "- 15 sequenced vectors from doc2vec as input,\n",
        "- the next vector (16th) as output.\n",
        "\n",
        "so, the dimension of X_train must be **(number of sequences, 15, 500)** and the dimension of y_train: **(number of sequences, 500)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7k2XocCzF0-",
        "colab_type": "code",
        "colab": {},
        "outputId": "8ed984a2-2db5-4941-e69a-989a8ddc6c63"
      },
      "source": [
        "nb_sequenced_sentences = 15\n",
        "vector_dim = 500\n",
        "\n",
        "X_train = np.zeros((len(sentences), nb_sequenced_sentences, vector_dim), dtype=np.float)\n",
        "y_train = np.zeros((len(sentences), vector_dim), dtype=np.float)\n",
        "\n",
        "t = 1000\n",
        "for i in range(len(sentences_label)-nb_sequenced_sentences-1):\n",
        "    if i % t == 0: print(\"new sequence: \", i)\n",
        "    \n",
        "    for k in range(nb_sequenced_sentences):\n",
        "        sent = sentences_label[i+k]\n",
        "        vect = sentences_vector[i+k]\n",
        "        \n",
        "        if i % t == 0:\n",
        "            print(\"  \", k + 1 ,\"th vector for this sequence. Sentence \", sent, \"(vector dim = \", len(vect), \")\")\n",
        "            \n",
        "        for j in range(len(vect)):\n",
        "            X_train[i, k, j] = vect[j]\n",
        "    \n",
        "    senty = sentences_label[i+nb_sequenced_sentences]\n",
        "    vecty = sentences_vector[i+nb_sequenced_sentences]\n",
        "    if i % t == 0: print(\"  y vector for this sequence \", senty, \": (vector dim = \", len(vecty), \")\")\n",
        "    for j in range(len(vecty)):\n",
        "        y_train[i, j] = vecty[j]\n",
        "\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new sequence:  0\n",
            "   1 th vector for this sequence. Sentence  ID0 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID1 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID2 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID3 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID4 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID5 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID6 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID7 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID8 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID9 (vector dim =  500 )\n",
            "   11 th vector for this sequence. Sentence  ID10 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID11 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID12 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID13 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID14 (vector dim =  500 )\n",
            "  y vector for this sequence  ID15 : (vector dim =  500 )\n",
            "new sequence:  1000\n",
            "   1 th vector for this sequence. Sentence  ID1000 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID1001 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID1002 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID1003 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID1004 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID1005 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID1006 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID1007 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID1008 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID1009 (vector dim =  500 )\n",
            "   11 th vector for this sequence. Sentence  ID1010 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID1011 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID1012 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID1013 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID1014 (vector dim =  500 )\n",
            "  y vector for this sequence  ID1015 : (vector dim =  500 )\n",
            "new sequence:  2000\n",
            "   1 th vector for this sequence. Sentence  ID2000 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID2001 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID2002 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID2003 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID2004 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID2005 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID2006 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID2007 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID2008 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID2009 (vector dim =  500 )\n",
            "   11 th vector for this sequence. Sentence  ID2010 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID2011 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID2012 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID2013 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID2014 (vector dim =  500 )\n",
            "  y vector for this sequence  ID2015 : (vector dim =  500 )\n",
            "new sequence:  3000\n",
            "   1 th vector for this sequence. Sentence  ID3000 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID3001 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID3002 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID3003 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID3004 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID3005 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID3006 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID3007 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID3008 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID3009 (vector dim =  500 )\n",
            "   11 th vector for this sequence. Sentence  ID3010 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID3011 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID3012 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID3013 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID3014 (vector dim =  500 )\n",
            "  y vector for this sequence  ID3015 : (vector dim =  500 )\n",
            "new sequence:  4000\n",
            "   1 th vector for this sequence. Sentence  ID4000 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID4001 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID4002 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID4003 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID4004 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID4005 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID4006 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID4007 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID4008 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID4009 (vector dim =  500 )\n",
            "   11 th vector for this sequence. Sentence  ID4010 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID4011 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID4012 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID4013 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID4014 (vector dim =  500 )\n",
            "  y vector for this sequence  ID4015 : (vector dim =  500 )\n",
            "new sequence:  5000\n",
            "   1 th vector for this sequence. Sentence  ID5000 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID5001 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID5002 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID5003 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID5004 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID5005 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID5006 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID5007 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID5008 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID5009 (vector dim =  500 )\n",
            "   11 th vector for this sequence. Sentence  ID5010 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID5011 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID5012 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID5013 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID5014 (vector dim =  500 )\n",
            "  y vector for this sequence  ID5015 : (vector dim =  500 )\n",
            "new sequence:  6000\n",
            "   1 th vector for this sequence. Sentence  ID6000 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID6001 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID6002 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID6003 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID6004 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID6005 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID6006 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID6007 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID6008 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID6009 (vector dim =  500 )\n",
            "   11 th vector for this sequence. Sentence  ID6010 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID6011 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID6012 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID6013 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID6014 (vector dim =  500 )\n",
            "  y vector for this sequence  ID6015 : (vector dim =  500 )\n",
            "new sequence:  7000\n",
            "   1 th vector for this sequence. Sentence  ID7000 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID7001 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID7002 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID7003 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID7004 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID7005 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID7006 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID7007 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID7008 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID7009 (vector dim =  500 )\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "   11 th vector for this sequence. Sentence  ID7010 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID7011 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID7012 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID7013 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID7014 (vector dim =  500 )\n",
            "  y vector for this sequence  ID7015 : (vector dim =  500 )\n",
            "new sequence:  8000\n",
            "   1 th vector for this sequence. Sentence  ID8000 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID8001 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID8002 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID8003 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID8004 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID8005 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID8006 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID8007 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID8008 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID8009 (vector dim =  500 )\n",
            "   11 th vector for this sequence. Sentence  ID8010 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID8011 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID8012 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID8013 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID8014 (vector dim =  500 )\n",
            "  y vector for this sequence  ID8015 : (vector dim =  500 )\n",
            "new sequence:  9000\n",
            "   1 th vector for this sequence. Sentence  ID9000 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID9001 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID9002 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID9003 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID9004 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID9005 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID9006 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID9007 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID9008 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID9009 (vector dim =  500 )\n",
            "   11 th vector for this sequence. Sentence  ID9010 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID9011 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID9012 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID9013 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID9014 (vector dim =  500 )\n",
            "  y vector for this sequence  ID9015 : (vector dim =  500 )\n",
            "new sequence:  10000\n",
            "   1 th vector for this sequence. Sentence  ID10000 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID10001 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID10002 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID10003 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID10004 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID10005 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID10006 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID10007 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID10008 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID10009 (vector dim =  500 )\n",
            "   11 th vector for this sequence. Sentence  ID10010 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID10011 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID10012 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID10013 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID10014 (vector dim =  500 )\n",
            "  y vector for this sequence  ID10015 : (vector dim =  500 )\n",
            "new sequence:  11000\n",
            "   1 th vector for this sequence. Sentence  ID11000 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID11001 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID11002 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID11003 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID11004 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID11005 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID11006 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID11007 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID11008 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID11009 (vector dim =  500 )\n",
            "   11 th vector for this sequence. Sentence  ID11010 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID11011 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID11012 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID11013 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID11014 (vector dim =  500 )\n",
            "  y vector for this sequence  ID11015 : (vector dim =  500 )\n",
            "new sequence:  12000\n",
            "   1 th vector for this sequence. Sentence  ID12000 (vector dim =  500 )\n",
            "   2 th vector for this sequence. Sentence  ID12001 (vector dim =  500 )\n",
            "   3 th vector for this sequence. Sentence  ID12002 (vector dim =  500 )\n",
            "   4 th vector for this sequence. Sentence  ID12003 (vector dim =  500 )\n",
            "   5 th vector for this sequence. Sentence  ID12004 (vector dim =  500 )\n",
            "   6 th vector for this sequence. Sentence  ID12005 (vector dim =  500 )\n",
            "   7 th vector for this sequence. Sentence  ID12006 (vector dim =  500 )\n",
            "   8 th vector for this sequence. Sentence  ID12007 (vector dim =  500 )\n",
            "   9 th vector for this sequence. Sentence  ID12008 (vector dim =  500 )\n",
            "   10 th vector for this sequence. Sentence  ID12009 (vector dim =  500 )\n",
            "   11 th vector for this sequence. Sentence  ID12010 (vector dim =  500 )\n",
            "   12 th vector for this sequence. Sentence  ID12011 (vector dim =  500 )\n",
            "   13 th vector for this sequence. Sentence  ID12012 (vector dim =  500 )\n",
            "   14 th vector for this sequence. Sentence  ID12013 (vector dim =  500 )\n",
            "   15 th vector for this sequence. Sentence  ID12014 (vector dim =  500 )\n",
            "  y vector for this sequence  ID12015 : (vector dim =  500 )\n",
            "(12273, 15, 500) (12273, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKw8tn2NzF1F",
        "colab_type": "text"
      },
      "source": [
        "# 3. Create the Keras Model\n",
        "\n",
        "Great, let's create the model now…\n",
        "\n",
        "First, we load the library and create the function to define a simple keras Model:\n",
        "\n",
        "- bidirectional LSTM,\n",
        "- with size of 512 and using RELU as activation (very small, but quicker to perform the test),\n",
        "- then a dropout layer of 0,5.\n",
        "- The network will not provide me a probability but directly the next vector for a given sequence. So I finish it with a simple dense layer of the size of the vector dimension.\n",
        "\n",
        "I use ADAM as optimizer and the loss calculation is done using **logcosh**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ad1AMBtBzF1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Dropout, Embedding, Flatten, Bidirectional, Input, LSTM\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import categorical_accuracy, mean_squared_error, mean_absolute_error, logcosh\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "def bidirectional_lstm_model(seq_length, vector_dim):\n",
        "    print('Building LSTM model...')\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vector_dim)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(vector_dim))\n",
        "    \n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
        "    model.compile(loss='logcosh', optimizer=optimizer, metrics=['acc'])\n",
        "    print('LSTM model built.')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuNYv3uHzF1O",
        "colab_type": "text"
      },
      "source": [
        "Then we create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IubcRu4YzF1P",
        "colab_type": "code",
        "colab": {},
        "outputId": "466bd863-ad2c-4ece-d91e-45561da279f5"
      },
      "source": [
        "rnn_size = 512 # size of RNN\n",
        "vector_dim = 500\n",
        "learning_rate = 0.0001 #learning rate\n",
        "\n",
        "model_sequence = bidirectional_lstm_model(nb_sequenced_sentences, vector_dim)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building LSTM model...\n",
            "LSTM model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsZaL3yczF1T",
        "colab_type": "text"
      },
      "source": [
        "And we train it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDAwxhhNzF1U",
        "colab_type": "code",
        "colab": {},
        "outputId": "5d107c31-59e9-4731-9a59-50cfc26e27a4"
      },
      "source": [
        "batch_size = 30 # minibatch size\n",
        "\n",
        "callbacks=[EarlyStopping(patience=3, monitor='val_loss'),\n",
        "           ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_sequence_lstm.{epoch:02d}.hdf5',\\\n",
        "                           monitor='val_loss', verbose=1, mode='auto', period=5)]\n",
        "\n",
        "history = model_sequence.fit(X_train, y_train,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True,\n",
        "                 epochs=40,\n",
        "                 callbacks=callbacks,\n",
        "                 validation_split=0.1)\n",
        "\n",
        "#save the model\n",
        "model_sequence.save(save_dir + \"/\" + 'my_model_sequence_lstm.final2.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11045 samples, validate on 1228 samples\n",
            "Epoch 1/40\n",
            "11045/11045 [==============================] - 153s - loss: 0.1219 - acc: 0.0896 - val_loss: 0.1122 - val_acc: 0.0798\n",
            "Epoch 2/40\n",
            "11045/11045 [==============================] - 151s - loss: 0.1136 - acc: 0.1015 - val_loss: 0.1116 - val_acc: 0.0863\n",
            "Epoch 3/40\n",
            "11045/11045 [==============================] - 152s - loss: 0.1123 - acc: 0.0960 - val_loss: 0.1114 - val_acc: 0.0945\n",
            "Epoch 4/40\n",
            "11045/11045 [==============================] - 152s - loss: 0.1116 - acc: 0.1051 - val_loss: 0.1113 - val_acc: 0.0912\n",
            "Epoch 5/40\n",
            "11040/11045 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.1035Epoch 00004: saving model to save/my_model_sequence_lstm.04.hdf5\n",
            "11045/11045 [==============================] - 154s - loss: 0.1110 - acc: 0.1036 - val_loss: 0.1113 - val_acc: 0.0912\n",
            "Epoch 6/40\n",
            "11045/11045 [==============================] - 150s - loss: 0.1107 - acc: 0.1044 - val_loss: 0.1112 - val_acc: 0.0912\n",
            "Epoch 7/40\n",
            "11045/11045 [==============================] - 474s - loss: 0.1103 - acc: 0.1091 - val_loss: 0.1114 - val_acc: 0.0936\n",
            "Epoch 8/40\n",
            "11045/11045 [==============================] - 144s - loss: 0.1099 - acc: 0.1111 - val_loss: 0.1112 - val_acc: 0.0953\n",
            "Epoch 9/40\n",
            "11045/11045 [==============================] - 144s - loss: 0.1095 - acc: 0.1182 - val_loss: 0.1114 - val_acc: 0.0945\n",
            "Epoch 10/40\n",
            "11040/11045 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.1178Epoch 00009: saving model to save/my_model_sequence_lstm.09.hdf5\n",
            "11045/11045 [==============================] - 176s - loss: 0.1091 - acc: 0.1177 - val_loss: 0.1112 - val_acc: 0.0945\n",
            "Epoch 11/40\n",
            "11045/11045 [==============================] - 167s - loss: 0.1086 - acc: 0.1227 - val_loss: 0.1113 - val_acc: 0.1002\n",
            "Epoch 12/40\n",
            "11045/11045 [==============================] - 158s - loss: 0.1082 - acc: 0.1193 - val_loss: 0.1114 - val_acc: 0.0985\n",
            "Epoch 13/40\n",
            "11045/11045 [==============================] - 160s - loss: 0.1078 - acc: 0.1241 - val_loss: 0.1115 - val_acc: 0.0912\n",
            "Epoch 14/40\n",
            "11045/11045 [==============================] - 150s - loss: 0.1073 - acc: 0.1273 - val_loss: 0.1116 - val_acc: 0.0993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ene-2gjFzF1d",
        "colab_type": "text"
      },
      "source": [
        "Great ! After few hours of training, we have trained a model to predict the next best sentence vector for a given sequence of sentences.\n",
        "\n",
        "Few remarks regarding the results:\n",
        "\n",
        "the loss drop to 0.1049, the accuracy is around 14%,\n",
        "the val_loss is around 0.1064 with val accuracy around 16%.\n",
        "\n",
        "# 4. Conclusion\n",
        "As you probably noticed, the raw result of the neural networks trained during the tutorial is not \"amazing\"… We can probably do better.\n",
        "\n",
        "However, let's check if the exercice is good enough to select the best next sentence of a text. I hope it will be fair enough for my test, indeed, for a given sequence of sentences, there is no clear determinism in the sequence to be chosen.\n",
        "\n",
        "In order to test that, we have to, for a given sequence of sentences:\n",
        "\n",
        "- generate, using our **first LSTM model**, different candidates of sentences,\n",
        "- Infer their vectors using our **doc2vec model**,\n",
        "- Generate, using our **second LSTM model**, the best following vector for the sequence,\n",
        "- then select the most similar vector.\n",
        "- That’s what I’ll try to do in the next part of this experiment…\n",
        "\n",
        "Thanks for reading !"
      ]
    }
  ]
}